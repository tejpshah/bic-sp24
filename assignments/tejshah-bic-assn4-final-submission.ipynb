{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAarlyo_pahq"
      },
      "source": [
        "# Learning Goals\n",
        "The goal of this assignment is to use the knowledge gained in the course to develop an end-to-end SNN for MNIST digits classification trained using state-of-the-art gradient-descent algorithm. Along the way, you will also learn the basics of developing any machine learning application- how to handle data using data loaders; defining and optimizing loss; evaluating an algorithm using validation set; speeding up training using GPUs. You will also learn the basics of programming using PyTorch which is by far the most widely used library for machine learning research- being used in applications such as autonomous driving, robotic control, cancer research, and much more!\n",
        "\n",
        "Let's import all the libraries required for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3-Y9vLFQpahs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUcSLe5Opaht"
      },
      "source": [
        "# Question 1: Limitation of backprop for SNN\n",
        "\n",
        "## 1a.\n",
        "Sketch out the algorithm for training an SNN using backpropagation. Your algorithm should describe when and how the weights are updated. What is the main limitation of using backpropagation for training an SNN? Describe a solution to resolve the limitation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTnIUY0vpaht"
      },
      "source": [
        "## Answer 1a.\n",
        "\n",
        "- Initialize weights $W$ randomly, learning rate $\\eta$, and LIF Neuron parameters (i.e. $V_{threshold}$).\n",
        "- Compute membrane potentials, firing rates, and generate output spike trains\n",
        "- Calculate loss $L$ based on the required output spike train at the end of network\n",
        "- Compute $\\frac{\\partial L}{\\partial W}$ and update all the weight of the network with respect to the loss, using the chain rule from calculus\n",
        "\n",
        "The weights are updated right after the forward pass through the network is completed. The main limitations of backpopogation for training an SNN has to come down to the fact that spikes by inherent nature are discrete, not continuous and differentiable. Therefore, we need to approximate the spiking behavior with a surrogate gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwMDS1Lbpaht"
      },
      "source": [
        "## 1b.\n",
        "In this exercise, we will implement the solution for overcoming backprop limitation for SNN. First the preliminaries: In PyTorch, arrays are called tensors. Gradients are computed automatically using the automatic differentiation package (autograd). The main elements of an autograd function are the forward and backward functions. The forward function simply performs the forward pass, i.e. computing output tensors from the input tensors. The backward function receives the gradient of the output tensors w.r.t. some scalar value, and computes the gradient of the input tensors w.r.t. the same scalar value.\n",
        "\n",
        "Below, we define the autograd function class for pseudo-gradient using the rectangular function. Most of the implementation is already written for you. Your task is to fill two key components- i) in the forward function, write the implementation for generating spike outputs from the inputs; ii) in the backward function, write the implementation for computing the gradient of the spike using rectangular psuedo-grad function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tl1XLBYdpaht"
      },
      "outputs": [],
      "source": [
        "class PseudoSpikeRect(torch.autograd.Function):\n",
        "    \"\"\" Rectangular Pseudo-grad function \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, vth, grad_win, grad_amp):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input (Torch Tensor): Input tensor containing voltages of neurons in a layer\n",
        "            vth (Float): Voltage threshold for spiking\n",
        "            grad_win (Float): Window for computing pseudogradient\n",
        "            grad_amp (Float): Amplification factor for the gradients\n",
        "\n",
        "        Returns:\n",
        "            output (Torch Tensor): Generated spikes for the input\n",
        "\n",
        "        Write the operation for computing the output spikes from the input. The operation should be vectorized, i.e. no loops.\n",
        "        \"\"\"\n",
        "\n",
        "        #Saving variables for backward pass. Nothing to do here\n",
        "        ctx.save_for_backward(input)\n",
        "        ctx.vth = vth\n",
        "        ctx.grad_win = grad_win\n",
        "        ctx.grad_amp = grad_amp\n",
        "\n",
        "        #Compute output from the input. No loops. Hint: Use Pytorch \"greater than\" function.\n",
        "        output = (input > vth).float()\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            grad_output (Torch Tensor): Gradient of the output\n",
        "\n",
        "        Returns:\n",
        "            grad (Torch Tensor): Gradient of the input\n",
        "\n",
        "        Write the operation for computing the output spikes from the input. The operation should be vectorized, i.e. no loops.\n",
        "        \"\"\"\n",
        "\n",
        "        #Retrieving variables from forward pass. Nothing to do here.\n",
        "        input, = ctx.saved_tensors\n",
        "        vth = ctx.vth\n",
        "        grad_win = ctx.grad_win\n",
        "        grad_amp = ctx.grad_amp\n",
        "        grad_input = grad_output.clone()\n",
        "\n",
        "        #compute the gradient of the input using rectangular pseudograd function\n",
        "        spike_pseudo_grad = ((input > (vth - grad_win / 2)) & (input < (vth + grad_win / 2))).float()\n",
        "\n",
        "        #Multiplying by gradient amplifier. Nothing to do here\n",
        "        grad = grad_amp * grad_input * spike_pseudo_grad.float()\n",
        "        return grad, None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNaomy8ipahu"
      },
      "source": [
        "# Question 2: Creating a Layer\n",
        "\n",
        "In this exercise, we will create the class definition for a layer of LIF neurons (similar to the implementation in Assignment 2).\n",
        "\n",
        "Below is the class definition of an LIF neuron layer. Your task is to write the operation for integrating the presynaptic spikes into voltage, and then transforming to spikes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2jy9JPIWpahu"
      },
      "outputs": [],
      "source": [
        "class LinearIFCell(nn.Module):\n",
        "    \"\"\" Leaky Integrate-and-fire neuron layer\"\"\"\n",
        "\n",
        "    def __init__(self, psp_func, pseudo_grad_ops, param):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            psp_func (Torch Function): Pre-synaptic function\n",
        "            pseudo_grad_ops (Torch Function): Pseudo-grad function\n",
        "            param (tuple): Cell parameters (Voltage Threshold, gradient window, gradient amplitude)\n",
        "\n",
        "        This function is complete. You do not need to do anything here.\n",
        "        \"\"\"\n",
        "        super(LinearIFCell, self).__init__()\n",
        "        self.psp_func = psp_func\n",
        "        self.pseudo_grad_ops = pseudo_grad_ops\n",
        "        self.vdecay, self.vth, self.grad_win, self.grad_amp = param\n",
        "\n",
        "    def forward(self, input_data, state):\n",
        "        \"\"\"\n",
        "        Forward function\n",
        "        Args:\n",
        "            input_data (Tensor): input spike from pre-synaptic neurons\n",
        "            state (tuple): output spike of last timestep and voltage of last timestep\n",
        "        Returns:\n",
        "            output: output spike\n",
        "            state: updated neuron states\n",
        "\n",
        "        Write the operation for integrating the presynaptic spikes into voltage.\n",
        "        \"\"\"\n",
        "        pre_spike, pre_volt = state\n",
        "\n",
        "        #Compute the voltage from the presynaptic inputs. This should be a vectorized operation. No loops.\n",
        "        volt = self.vdecay * pre_volt + self.psp_func(input_data)\n",
        "\n",
        "        #Compute the spike output by using the pseudo_grad_ops function. This should be a vectorized operation. No loops.\n",
        "        output = self.pseudo_grad_ops(volt, self.vth, self.grad_win, self.grad_amp)\n",
        "\n",
        "        return output, (output, volt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4rDMfDlpahu"
      },
      "source": [
        "# Question 3: Creating a Network\n",
        "\n",
        "## 3a.\n",
        "We will now create a one-layer SNN using the class definitions above. Preliminaries: In Assignment 2, the psp was computed using numpy matrix multiplication of weights and inputs. In PyTorch, nn.Linear() achieves the same. You can find the documentation here: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html. We will use this class to serve as our psp function required for creating a layer according to the implementation in Q2.\n",
        "\n",
        "Below is the class definition of a network. Your task is to fill in the required components in the init and forward functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wa6FtcbFpahu"
      },
      "outputs": [],
      "source": [
        "class SingleHiddenLayerSNN(nn.Module):\n",
        "    \"\"\" SNN with single hidden layer \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, param_dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): input dimension\n",
        "            output_dim (int): output dimension\n",
        "            hidden_dim (int): hidden layer dimension\n",
        "            param_dict (dict): neuron parameter dictionary for each layer (Voltage Threshold, gradient window, gradient amplitude)\n",
        "\n",
        "        Create hidden and output layers using implementation of the layer in Q2. and using nn.Linear as the psp function.\n",
        "        \"\"\"\n",
        "        super(SingleHiddenLayerSNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        pseudo_grad_ops = PseudoSpikeRect.apply\n",
        "\n",
        "        #Create the hidden layer. Assume that the hidden layer neuron parameters are in param_dict['hid_layer']. Set bias=False for nn.Linear.\n",
        "        self.hidden_cell = LinearIFCell(nn.Linear(input_dim, hidden_dim, bias=False), pseudo_grad_ops, param_dict['hid_layer'])\n",
        "\n",
        "        #Create the output layer. Output layer params are in param_dict['out_layer']. Set bias=False for nn.Linear.\n",
        "        self.output_cell = LinearIFCell(nn.Linear(hidden_dim, output_dim, bias=False), pseudo_grad_ops, param_dict['out_layer'])\n",
        "\n",
        "    def forward(self, spike_data, init_states_dict, batch_size, spike_ts):\n",
        "        \"\"\"\n",
        "        Forward function\n",
        "        Args:\n",
        "            spike_data (Tensor): spike data input (batch_size, input_dim, spike_ts)\n",
        "            init_states_dict (dict): initial states for each layer- 'hid_layer' for hidden layer; 'out_layer' for output layer.\n",
        "            batch_size (int): batch size\n",
        "            spike_ts (int): spike timesteps\n",
        "        Returns:\n",
        "            output: number of spikes of output layer\n",
        "\n",
        "        Write the operations for propagating the input through the network and computing the spike outputs.\n",
        "        \"\"\"\n",
        "        hidden_state, out_state = init_states_dict['hid_layer'], init_states_dict['out_layer']\n",
        "        spike_data_flatten = spike_data.view(batch_size, self.input_dim, spike_ts)\n",
        "        output_list = [] #List to store the output at each timestep\n",
        "        for tt in range(spike_ts):\n",
        "            #Retrieve the input at time tt\n",
        "            input_data = spike_data_flatten[:, :, tt]\n",
        "\n",
        "            #Propagate through the hidden layer\n",
        "            hidden_out, hidden_state = self.hidden_cell(input_data, hidden_state)\n",
        "\n",
        "            #Propagate through the output layer\n",
        "            out_spike, out_state = self.output_cell(hidden_out, out_state)\n",
        "\n",
        "            #Append output spikes to output list\n",
        "            output_list.append(out_spike)\n",
        "\n",
        "        #Sum the outputs to compute spike count for each output neuron. Torch.stack and Torch.sum might be useful here. No loops\n",
        "        output = torch.sum(torch.stack(output_list, dim=2), dim=2)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFX0_dv4pahu"
      },
      "source": [
        "## 3b.\n",
        "Next, we need a Wrapper class that: i) Initializes the parameters required for creating the SNN class object; ii) creates the SNN class objects using the initial parameters; iii) Computes the SNN output and returns it. The class is already written for you. You do not need to do anything here. Just understand the implementation so that you can use it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ly3kUZHVpahu"
      },
      "outputs": [],
      "source": [
        "class WrapSNN(nn.Module):\n",
        "    \"\"\" Wrapper of SNN \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, param_dict, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): input dimension\n",
        "            output_dim (int): output dimension\n",
        "            hidden_dim (int): hidden layer dimension\n",
        "            param_dict (dict): neuron parameter dictionary\n",
        "            device (device): device\n",
        "        \"\"\"\n",
        "        super(WrapSNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "        self.snn = SingleHiddenLayerSNN(input_dim, output_dim, hidden_dim, param_dict)\n",
        "\n",
        "    def forward(self, spike_data):\n",
        "        \"\"\"\n",
        "        Forward function\n",
        "        Args:\n",
        "            spike_data (Tensor): spike data input\n",
        "        Returns:\n",
        "            output: number of spikes of output layer\n",
        "        \"\"\"\n",
        "        batch_size = spike_data.shape[0]\n",
        "        spike_ts = spike_data.shape[-1]\n",
        "        init_states_dict = {}\n",
        "        # Hidden layer\n",
        "        hidden_volt = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
        "        hidden_spike = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
        "        init_states_dict['hid_layer'] = (hidden_spike, hidden_volt)\n",
        "        # Output layer\n",
        "        out_volt = torch.zeros(batch_size, self.output_dim, device=self.device)\n",
        "        out_spike = torch.zeros(batch_size, self.output_dim, device=self.device)\n",
        "        init_states_dict['out_layer'] = (out_spike, out_volt)\n",
        "        # SNN\n",
        "        output = self.snn(spike_data, init_states_dict, batch_size, spike_ts)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kazyhygopahv"
      },
      "source": [
        "# Question 4: Encoding MNIST into spikes\n",
        "Following is the function that converts an MNIST image into spikes. You have already implemented it using Numpy in Assignment 2. The implementation remains the same- except that we will now use Torch tensors instead of numpy arrays. Fill in the components to convert a batch of torch tensors into spikes. Since the goal is to learn writing optimized code using PyTorch, you are supposed to do this without any loops. Use vector operations instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1NsxZ4Gwpahv"
      },
      "outputs": [],
      "source": [
        "def img_2_event_img(image, device, spike_ts):\n",
        "    \"\"\"\n",
        "    Transform image to event image\n",
        "    Args:\n",
        "        image (Tensor): image\n",
        "        device (device): device (can be either CPU or GPU)\n",
        "        spike_ts (int): spike timestep\n",
        "    Returns:\n",
        "        event_image: event image\n",
        "    \"\"\"\n",
        "    batch_size = image.shape[0]\n",
        "    channel_size = image.shape[1]\n",
        "    image_size = image.shape[2]\n",
        "    image = image.view(batch_size, channel_size, image_size, image_size, 1)\n",
        "\n",
        "    #Create a random image of shape batch_size x channel_size x image_size x image_size x spike_ts. Torch rand function might be useful here.\n",
        "    #Remember to put the random image on the device specified in the function argument.\n",
        "    random_image = torch.rand(batch_size, channel_size, image_size, image_size, spike_ts, device=device)\n",
        "\n",
        "    #Generate event image using image and random image\n",
        "    event_image = (image > random_image).float()\n",
        "\n",
        "    return event_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oshz5oVSpahv"
      },
      "source": [
        "# Question 5: Training the SNN\n",
        "\n",
        "In this exercise, we will write the function to train an SNN using spatiotemporal backprop (stbp). A typical training loop works as follows:\n",
        "\n",
        "1. Split the dataset into train and test. The network is trained on all the batches in the train dataset, and then validated on the test dataset. This gives us an idea of how well the network generalizes to unseen data.\n",
        "\n",
        "2. A criterion is defined to compute the loss. For classification tasks, this is generally Cross Entropy (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "\n",
        "3. The network is initialized with random weights.\n",
        "\n",
        "4. Typically, the training is done on mini-batches of train data. This means that at every training instance, the network receives batches of training data where each batch contains small number of samples. The loss and the gradients required for updating the weights are averaged over these batches.\n",
        "\n",
        "5. Within every training iteration, first we retrieve the batches of data. We compute the network prediction. Then we compute the loss by comparing the network prediction against the true labels. Then the gradient of the loss with respect to all the network weights is computed. This gradient is then used to update the weights in the network.  \n",
        "\n",
        "Thankfully, PyTorch provides APIs to automate most of the above steps. Below is the function training an SNN that implements the algorithm presented above using PyTorch. Your task is to fill the components. Refer to the comments for hints on what PyTorch functions to use.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dzHkduoPpahv"
      },
      "outputs": [],
      "source": [
        "def stbp_snn_training(network, spike_ts, device, batch_size=128, test_batch_size=256, epoch=100):\n",
        "    \"\"\"\n",
        "    STBP SNN training\n",
        "    Args:\n",
        "        network (SNN): STBP learning SNN\n",
        "        spike_ts (int): spike timestep\n",
        "        device (device): device\n",
        "        batch_size (int): batch size for training\n",
        "        test_batch_size (int): batch size for testing\n",
        "        epoch (int): number of epochs\n",
        "    Returns:\n",
        "        train_loss_list: list of training loss for each epoch\n",
        "        test_accuracy_list: list of test accuracy for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    #Creating folder where MNIST data is saved. Loading the MNIST dataset. This code is complete. Do not touch it.\n",
        "    try:\n",
        "        os.mkdir(\"./data\")\n",
        "        print(\"Directory data Created\")\n",
        "    except FileExistsError:\n",
        "        print(\"Directory data already exists\")\n",
        "\n",
        "    data_path = './data/'\n",
        "    train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, download=True,\n",
        "                                               transform=transforms.ToTensor())\n",
        "    test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, download=True,\n",
        "                                              transform=transforms.ToTensor())\n",
        "\n",
        "    # Train and test dataloader\n",
        "\n",
        "    #Given the train and test datasets, we need to create dataloaders to load the datasets in the right format.\n",
        "    #You can read about PyTorch Dataset and Dataloaders here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "    #For now, this part is complete and you do not need to do anything here.\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                  shuffle=False, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size,\n",
        "                                 shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "    # Next we need to define a criteria for computing the loss.\n",
        "    # Refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html on how to define the cross entropy loss.\n",
        "    # Note that you just need to define the loss here (and not compute it)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #Define an optimizer that will perform the weight updates. You can find more about the optimizers in PyTorch here: https://pytorch.org/docs/stable/optim.html\n",
        "    #Taking the help of the documentation above, create an optimizer for stochastic gradient descent (SGD).\n",
        "    optimizer = optim.SGD(network.parameters(), lr=1e-3)\n",
        "\n",
        "    # List for saving loss and accuracy\n",
        "    train_loss_list, test_accuracy_list = [], []\n",
        "    test_num = len(test_dataset)\n",
        "\n",
        "    # Start training\n",
        "\n",
        "    #Put the network on the device (typically GPU but can also be cpu)\n",
        "    network.to(device)\n",
        "\n",
        "    #Loop for the epochs\n",
        "    for ee in range(epoch):\n",
        "        #Keep track of running loss\n",
        "        running_loss = 0.0\n",
        "        running_batch_num = 0\n",
        "        train_start = time.time()\n",
        "\n",
        "        #Iterate over the training data in train dataloader\n",
        "        for data in train_dataloader:\n",
        "\n",
        "            #Retrieve the image and label from data\n",
        "            image, label = data\n",
        "\n",
        "            #Put the image and labels on the device\n",
        "            image = image.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            #Convert images to event images\n",
        "            event_image = img_2_event_img(image, device, spike_ts)\n",
        "\n",
        "            #Before we backprop, we need to set the gradients for each tensor to zero. This is done using the zero_grad function in Pytorch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #Compute the network output for the event images\n",
        "            outputs = network(event_image)\n",
        "\n",
        "            #Compute the loss using the criterion defined previously. Store in a variable called loss\n",
        "            loss = criterion(outputs, label)\n",
        "\n",
        "            #Backpropagate the loss through the network. Use Pytorch backward() function: https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
        "            loss.backward()\n",
        "\n",
        "            #Update the network weights by taking an optimizer 'step'.\n",
        "            #You can learn how to do that here: https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step\n",
        "            optimizer.step()\n",
        "\n",
        "            #Updating tracking variables. Nothing to do here\n",
        "            running_loss += loss.item()\n",
        "            running_batch_num += 1\n",
        "        train_end = time.time()\n",
        "        train_loss_list.append(running_loss / running_batch_num)\n",
        "        print(\"Epoch %d Training Loss %.4f\" % (ee, train_loss_list[-1]), end=\" \")\n",
        "\n",
        "\n",
        "        #This ends one training iteration. After every training iteration, we can evaluate how well the network does on data that it has not seen before.\n",
        "        #This step is called testing and is done on test dataset.\n",
        "\n",
        "        #Counter to keep track of the number of correct predictions\n",
        "        test_correct_num = 0\n",
        "        test_start = time.time()\n",
        "        with torch.no_grad():\n",
        "            for data in test_dataloader:\n",
        "\n",
        "                #Retrieve the image and label from test data\n",
        "                image, label = data\n",
        "\n",
        "                #Put the image and labels on the device\n",
        "                image = image.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                #Convert the image into event images\n",
        "                event_image = img_2_event_img(image, device, spike_ts)\n",
        "\n",
        "                #Compute the network predictions and store in a variable called outputs\n",
        "                outputs = network(event_image)\n",
        "\n",
        "                #Get the class label as the largest activation. This is complete. Nothing to do here.\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                #Compare the network predictions against the true labels and update the counter for correct predictions. No loops.\n",
        "                test_correct_num += (predicted == label).sum().item()\n",
        "\n",
        "\n",
        "        #Updating tracking variables. Nothing to do here\n",
        "        test_end = time.time()\n",
        "        test_accuracy_list.append(test_correct_num / test_num)\n",
        "        print(\"Test Accuracy %.4f Training Time: %.1f Test Time: %.1f\" % (\n",
        "            test_accuracy_list[-1], train_end - train_start, test_end - test_start))\n",
        "\n",
        "    #Return the loss and accuracies. Nothing to do here.\n",
        "    print(\"End Training\")\n",
        "    network.to('cpu')\n",
        "    return train_loss_list, test_accuracy_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNP6tW-ipahv"
      },
      "source": [
        "Now we have everything ready to train and test our SNN using backprop. All that is left to do is initialize the network with the right parameters and call the training function on it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GVPImhGSpahv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea42117-1718-44bc-b468-d2da48ea78a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory data Created\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 128709004.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 30925630.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 80021884.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22760488.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Training Loss 1.7949 Test Accuracy 0.4051 Training Time: 18.4 Test Time: 1.7\n",
            "Epoch 1 Training Loss 1.4533 Test Accuracy 0.4505 Training Time: 15.0 Test Time: 1.7\n",
            "Epoch 2 Training Loss 1.3716 Test Accuracy 0.5202 Training Time: 16.4 Test Time: 2.0\n",
            "Epoch 3 Training Loss 1.1629 Test Accuracy 0.6275 Training Time: 15.2 Test Time: 1.7\n",
            "Epoch 4 Training Loss 1.0207 Test Accuracy 0.6285 Training Time: 15.9 Test Time: 1.9\n",
            "Epoch 5 Training Loss 0.9633 Test Accuracy 0.6344 Training Time: 16.8 Test Time: 1.8\n",
            "Epoch 6 Training Loss 0.9235 Test Accuracy 0.6416 Training Time: 15.3 Test Time: 1.7\n",
            "Epoch 7 Training Loss 0.8902 Test Accuracy 0.6498 Training Time: 15.6 Test Time: 2.4\n",
            "Epoch 8 Training Loss 0.8062 Test Accuracy 0.7294 Training Time: 15.3 Test Time: 1.7\n",
            "Epoch 9 Training Loss 0.7060 Test Accuracy 0.7469 Training Time: 15.4 Test Time: 1.7\n",
            "End Training\n"
          ]
        }
      ],
      "source": [
        "# Define the device on which training will be performed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Define the input dimensions in a variable\n",
        "input_dim = 28*28\n",
        "\n",
        "#Define the output dimensions in a variable\n",
        "output_dim = 10\n",
        "\n",
        "#Define the hidden layer dimension in a variable\n",
        "hidden_dim = 1024\n",
        "\n",
        "#Create a dictionary of the neuron parameters for the hidden and output layer. The keys should be 'hid_layer' and 'out_layer'.\n",
        "#The values of the dictionary should be a list of the neuron parameters for each layer where the list elements are [vdecay, vth, grad_win, grad_amp]\n",
        "param_dict = {\n",
        "    'hid_layer': [0.8, 0.50, 0.7, 1.0],\n",
        "    'out_layer': [0.8, 0.65, 0.7, 1.0]\n",
        "}\n",
        "\n",
        "#Define snn timesteps in a variable\n",
        "spike_ts = 20\n",
        "\n",
        "#Create the SNN using the class definition in 3b and the arguments defined above\n",
        "snn = WrapSNN(input_dim, output_dim, hidden_dim, param_dict, device)\n",
        "\n",
        "#Define the following training parameters\n",
        "#Batch size for training\n",
        "batch_size = 128\n",
        "\n",
        "#Batch size for testing\n",
        "test_batch_size = 128\n",
        "\n",
        "#Epochs\n",
        "epochs = 10\n",
        "\n",
        "#Train the snn using the above arguments and the definition in Q5.\n",
        "train_loss, test_accuracy = stbp_snn_training(snn, spike_ts, device, batch_size, test_batch_size, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JoAMl0opahv"
      },
      "source": [
        "# Question 6: Concluding Remarks\n",
        "If you have been able to implement all the parts in Q.1.-Q.5., you can now feel proud of yourself for having learned the basics of a deep learning pipeline that powers all the modern AI applications. Of course, the exact implementation varies in the input and network design for more complex applications, but the pipeline remains the same. In fact, we went beyond the conventional deep learning, and implemented a spiking deep learning algorithm which is not a skill that many people possess.\n",
        "\n",
        "Now that you know the basic implementation, there are lots of directions which you can pursue if you are interested in this research area- how do the hyper-parameters such as the pseudo-gradient window affect the network training process? How much training data do you need to achieve good performance? How does the amount of training data vary with the complexity of the task? To what limits can you push the spike encoding, i.e. what is the minimum timesteps you need to achieve good performance?\n",
        "\n",
        "You do not need to answer these questions in this assignment but it is something to think about if you are interested.\n",
        "\n",
        "**As the concluding question of the course, can you describe what are the three key lessons you learned from the course and why is it important to learn them?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UspW4c4cpahv"
      },
      "source": [
        "## Answer 6\n",
        "\n",
        "1. The first lesson I learned is that function follows form. As Michmizos taught in his 1st lecture, by fixing the exact human eye movement, we see emergent phenomena of saccades. Our brain is an existence proof for intelligence. Therefore, if we can mechanistically encode certain processes and their structure into algorithms, we should theoretically be able to replicate the same behavior in machines.\n",
        "\n",
        "2. The second lesson I learned is that there is a tradeoff frontier between biologically plausibility and computational efficiency. It's important to learn this lesson because we have to keep in mind inevitable tradeoffs as we pursue more brain-inspired algorithms.\n",
        "\n",
        "3. The third lesson I learned is that biological neural networks are really complex, and it's hard to precisely model them! Before taking this class, I had a lot of prior experience with deep learning. So, I thought that we could address many limitations simply by taking a biologically inspired approach (i.e. for adversarial robustness). Soon, though, I realized this approach is difficult to implement, though it may be fruitful. Biological inspiration is not a panacea but it is a promising direction for more capable algorithms."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}