{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_AqTbmJoe7B"
      },
      "source": [
        "# Learning Goals\n",
        "In Assignment 2, we learnt how to construct networks of spiking neurons and propagate information through a network of fixed weights. In this assignment, you will learn how to train network weights for a given task using brain-inspired learning rules.\n",
        "\n",
        "Let's import all the libraries required for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PNQaiLLwoe7C"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3R2XzELoe7D"
      },
      "source": [
        "# Question 1: Training a Network\n",
        "\n",
        "## 1a.\n",
        "What is the purpose of a learning algorithm? In other words, what does a learning algorithm dictate, and what is the objective of it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKeVXlZroe7D"
      },
      "source": [
        "## Answer 1a.\n",
        "The purpose of a learning algorithm is to push information from data into the structure of the network. The learning algorithm dictates \"how\" that information is computed by modulating parameters in the network. The objective function is usually a form of \"loss minimization\" (i.e. cross entropy loss). The overall goal is to build a suitable model during training which is generalizable to unseen data during testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUM1K8-coe7D"
      },
      "source": [
        "## 1b.\n",
        "Categorize and explain the various learning algorithms w.r.t. biological plausibility. Can you explain the tradeoffs involved with the different learning rules? *Hint: Think computational advantages and disadvantages of biological plausibility.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnPU7E-Doe7D"
      },
      "source": [
        "## Answer 1b.\n",
        "\n",
        "- High Biological Plausability: Hebbian Learning\n",
        "  - \"Neurons that fire together wire together\" which mirrors synaptic plasticity in biology. This is a simple learning rule, that explains unsupervised feature detection and pattern recognition. But, this approach fails with complicated temporal sequences.\n",
        "- High Biological Plausability: Spike-Timing Dependent Plasticity\n",
        "  - We have the Hebbian Learning idea but we also add in the idea of timing the neuronal spikes appropriately, which is more biologically plausible. We can get temporal dynamics at the cost of increased computational cost.\n",
        "- Moderate Biological Plausibility: Reinforcement Learning\n",
        "  - Reward prediction error is similar to dopaminergic signaling in the brain. RL captures agentic behavior as is classical in a MDP; however, RL is computationally complex, and may not capture full complexity of biological systems in smaller, simplified networks.\n",
        "- Low Biological Plausability: Backpropogation\n",
        "  - Learns by using error signal and propogates that through the network via the chain rule in calculus. This works well for industry-scale models; however, it's not biologically plausible (i.e. brain uses local updates but backprop uses a global update signal, backprop uses both a forward and backward pass which is not observed biologically).\n",
        "\n",
        "  Generally speaking, more biologically plausible means more computationally complex and less biologically plausible means less computationally complex.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa1rSy2noe7D"
      },
      "source": [
        "# Question 2: Hebbian Learning\n",
        "\n",
        "## 2a.\n",
        "\n",
        "In this exercise, you will implement the hebbian learning rule to solve AND Gate. First, we need to create a helper function to generate the training data. The function should return lists of tuples where each tuple comprises of numpy arrays of rate-coded inputs and the corresponding rate-coded output.\n",
        "\n",
        "Below is the function to generate the training data. Fill the components to return the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "85XJCcUZoe7D"
      },
      "outputs": [],
      "source": [
        "def genANDTrainData(snn_timestep):\n",
        "    \"\"\"\n",
        "    Function to generate the training data for AND\n",
        "        Args:\n",
        "            snn_timestep (int): timesteps for SNN simulation\n",
        "        Return:\n",
        "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
        "\n",
        "        Write the expressions for encoding 0 and 1. Then append all 4 cases of AND gate to the list train_data\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize an empty list for train data\n",
        "    train_data = []\n",
        "\n",
        "    #encode 0. Numpy random choice function might be useful here.\n",
        "    encode_0 = np.random.choice([0, 1], size=snn_timestep, p=[0.90, 0.10])\n",
        "\n",
        "    #encode 1. Numpy random choice function might be useful here.\n",
        "    encode_1 = np.random.choice([0, 1], size=snn_timestep, p=[0.10, 0.90])\n",
        "\n",
        "    #Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here.\n",
        "    train_data.append((np.stack((encode_0, encode_0)), np.zeros(snn_timestep)))\n",
        "    train_data.append((np.stack((encode_0, encode_1)), np.zeros(snn_timestep)))\n",
        "    train_data.append((np.stack((encode_1, encode_0)), np.zeros(snn_timestep)))\n",
        "    train_data.append((np.stack((encode_1, encode_1)), np.ones(snn_timestep)))\n",
        "\n",
        "    return train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge26J7HZoe7D"
      },
      "source": [
        "## 2b.\n",
        "We will use the implementation of the network from assignment 2 to create an SNN comprising of one input layer and one output layer. Can you explain algorithmically, how you can use this simple architecture to learn AND gate. Your algorithm should comprise of encoding, forward propagation, network training, and decoding steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bd8oIzNoe7E"
      },
      "source": [
        "## Answer 2b.\n",
        "\n",
        "Step 1 - Encoding:\n",
        "Generate training data $\\mathcal{D}$ for $\\{(0,0), (0,1), (1,0), (1,1)\\}$ using `genANDTrainData` function in 2A. This function gives us Spike In and Spike Out representations for all possible two-sized tuple binary combinations.\n",
        "\n",
        "Step 2 - Forward Propogation: Set up `SNN` object, initialized with an input dimension of 2 and an output dimension of 1. Also, set up a random `input_2_output_weight` matrix to be later updated during training.\n",
        "\n",
        "Step 3 - Network Training: Compute output neuron average firing rate and compare to input neuron firing rate. Increase the weights with the Hebbian Learning Rule if firing rates correspond.\n",
        "\n",
        "Step 4 - Decoding: Test the trained `SNN` model. After a model is encoded and passed through the model, aggregate output spikes to compute the firing rate and see how it compares to the average firing rate output neuron threshold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZimQ8m5oe7E"
      },
      "source": [
        "The SNN has already been implemented for you. You do not need to do anything here. Just understand the implementation so that you can use it in the later parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SMsMG3N_oe7E"
      },
      "outputs": [],
      "source": [
        "class LIFNeurons:\n",
        "    \"\"\"\n",
        "        Define Leaky Integrate-and-Fire Neuron Layer\n",
        "        This class is complete. You do not need to do anything here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dimension, vdecay, vth):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dimension (int): Number of LIF neurons in the layer\n",
        "            vdecay (float): voltage decay of LIF neurons\n",
        "            vth (float): voltage threshold of LIF neurons\n",
        "\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.vdecay = vdecay\n",
        "        self.vth = vth\n",
        "\n",
        "        # Initialize LIF neuron states\n",
        "        self.volt = np.zeros(self.dimension)\n",
        "        self.spike = np.zeros(self.dimension)\n",
        "\n",
        "    def __call__(self, psp_input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            psp_input (ndarray): synaptic inputs\n",
        "        Return:\n",
        "            self.spike: output spikes from the layer\n",
        "                \"\"\"\n",
        "        self.volt = self.vdecay * self.volt * (1. - self.spike) + psp_input\n",
        "        self.spike = (self.volt > self.vth).astype(float)\n",
        "        return self.spike\n",
        "\n",
        "class Connections:\n",
        "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
        "\n",
        "    def __init__(self, weights, pre_dimension, post_dimension):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            weights (ndarray): connection weights\n",
        "            pre_dimension (int): dimension for pre-synaptic neurons\n",
        "            post_dimension (int): dimension for post-synaptic neurons\n",
        "        \"\"\"\n",
        "        self.weights = weights\n",
        "        self.pre_dimension = pre_dimension\n",
        "        self.post_dimension = post_dimension\n",
        "\n",
        "    def __call__(self, spike_input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
        "        Return:\n",
        "            psp: postsynaptic layer activations\n",
        "        \"\"\"\n",
        "        psp = np.matmul(self.weights, spike_input)\n",
        "        return psp\n",
        "\n",
        "\n",
        "class SNN:\n",
        "    \"\"\" Define a Spiking Neural Network with No Hidden Layer \"\"\"\n",
        "\n",
        "    def __init__(self, input_2_output_weight,\n",
        "                 input_dimension=2, output_dimension=2,\n",
        "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer\n",
        "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer\n",
        "            input_dimension (int): input dimension\n",
        "            hidden_dimension (int): hidden_dimension\n",
        "            output_dimension (int): output_dimension\n",
        "            vdecay (float): voltage decay of LIF neuron\n",
        "            vth (float): voltage threshold of LIF neuron\n",
        "            snn_timestep (int): number of timesteps for inference\n",
        "        \"\"\"\n",
        "        self.snn_timestep = snn_timestep\n",
        "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
        "        self.input_2_output_connection = Connections(input_2_output_weight, input_dimension, output_dimension)\n",
        "\n",
        "    def __call__(self, spike_encoding):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            spike_encoding (ndarray): spike encoding of input\n",
        "        Return:\n",
        "            spike outputs of the network\n",
        "        \"\"\"\n",
        "        spike_output = np.zeros(self.output_layer.dimension)\n",
        "        for tt in range(self.snn_timestep):\n",
        "            input_2_output_psp = self.input_2_output_connection(spike_encoding[:, tt])\n",
        "            output_spikes = self.output_layer(input_2_output_psp)\n",
        "            spike_output += output_spikes\n",
        "        return spike_output/self.snn_timestep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQC9yEXWoe7E"
      },
      "source": [
        "## 2c.\n",
        "Next, you need to write a function for network training using hebbian learning rule. The function is defined below. You need to fill in the components so that the network weights are updated in the right manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Yp8gnjrioe7E"
      },
      "outputs": [],
      "source": [
        "def hebbian(network, train_data, lr=1e-5, epochs=10):\n",
        "    \"\"\"\n",
        "    Function to train a network using Hebbian learning rule\n",
        "        Args:\n",
        "            network (SNN): SNN network object\n",
        "            train_data (list): training data\n",
        "            lr (float): learning rate\n",
        "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples.\n",
        "\n",
        "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights.\n",
        "    \"\"\"\n",
        "\n",
        "    #iterate over the epochs\n",
        "    for ee in range(epochs):\n",
        "        #iterate over all samples in train_data\n",
        "        for data in train_data:\n",
        "\n",
        "            input_spikes, _ = data\n",
        "\n",
        "            #compute the firing rate for the input\n",
        "            input_fr = np.mean(input_spikes, axis=1)\n",
        "\n",
        "            #compute the firing rate for the output\n",
        "            output_fr = np.mean(network(input_spikes))\n",
        "\n",
        "            #compute the correlation using the firing rates calculated above\n",
        "            correlation = input_fr * output_fr\n",
        "\n",
        "            #compute the weight increment\n",
        "            weight_increment = lr * correlation\n",
        "\n",
        "            #increment the weight\n",
        "            network.input_2_output_connection.weights += weight_increment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA-LoxCtoe7F"
      },
      "source": [
        "## 2d.\n",
        "In this exercise, you will use your implementations above to train an SNN to learn AND gate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP1Ef2ufoe7F",
        "outputId": "a252df6a-fe9d-4f57-b69d-cbf3ed608d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "[[0.5507979  0.70814782]]\n",
            "\n",
            "Network outputs after training:\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.06]\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.46]\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.48]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Initial Weights:\n",
            "[[0.5639899  0.72116782]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "#Define a variable for input dimension\n",
        "input_dimension = 2\n",
        "\n",
        "#Define a variable for output dimension\n",
        "output_dimension = 1\n",
        "\n",
        "#Define a variable for voltage decay\n",
        "v_decay = 0.50\n",
        "\n",
        "#Define a variable for voltage threshold\n",
        "vth = 0.80\n",
        "\n",
        "#Define a variable for snn timesteps\n",
        "snn_timesteps = 50\n",
        "\n",
        "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
        "input_2_output_weight = np.random.rand(output_dimension,input_dimension)\n",
        "\n",
        "#print the initial weights\n",
        "print(\"Initial Weights:\")\n",
        "print(input_2_output_weight)\n",
        "\n",
        "#Initialize an snn using the arguments defined above\n",
        "snn = SNN(\n",
        "    input_2_output_weight,\n",
        "    input_dimension,\n",
        "    output_dimension,\n",
        "    v_decay,\n",
        "    vth,\n",
        "    snn_timesteps\n",
        ")\n",
        "\n",
        "#Get the training data for AND gate using the function defined in 2a.\n",
        "train_data = genANDTrainData(snn_timesteps)\n",
        "\n",
        "#Train the network using the function defined in 2c. with the appropriate arguments\n",
        "hebbian(\n",
        "    snn,\n",
        "    train_data,\n",
        "    lr=1e-3,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "#Test the trained network and print the network output for all 4 cases.\n",
        "print(\"\\nNetwork outputs after training:\")\n",
        "for data in train_data:\n",
        "    input_spikes, target_output = data\n",
        "    output = snn(input_spikes)\n",
        "    print(f\"Target: {target_output.mean()}, Predicted Output: {np.round(output)}, Actual Output: {output}\")\n",
        "\n",
        "#Print Final Network Weights\n",
        "print(\"Initial Weights:\")\n",
        "print(snn.input_2_output_connection.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gbcrbahoe7F"
      },
      "source": [
        "# Question 3: Limitations of Hebbian Learning rule\n",
        "\n",
        "## 3a.\n",
        "Can you learn the AND gate using 2 neurons in the output layer instead of one? If yes, describe what changes you might need to make to your algorithm in 2b. If not, explain why not, and what consequences it might entail for the use of hebbian learning for complex real-world tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AJi24UIoe7F"
      },
      "source": [
        "## Answer 3a.\n",
        "Yes, you can.\n",
        "- Step 1 - Encoding: Keep this the same.\n",
        "- Step 2 - Forward Propogation: Make `output_dimension=2` and size(`input_2_output_weight`)=`(2,2)`.\n",
        "- Step 3 - Network Training: There will be two neuron outputs. One neuron should fire for the `encoded(1,1)` case and for all other cases, the other neuron should fire. We can do this by making the correlation rule more fine-grained. We can increase weights for first output neuron if the input is (1,1) and increase weights for second output neuron if the input is anything else.\n",
        "- Step 4 - Decoding: If the 1st neuron fires, and the 2nd output doesn't then we predict 1 for the (1,1) case. In all other cases, we predict 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6XgeAIeoe7F"
      },
      "source": [
        "## 3b.\n",
        "Train the network using hebbian learning for AND gate with the same arguments as defined in 2d. but now multiply the number of epochs by 20. Can your network still learn AND gate correctly? Inspect the initial and final network weights, and compare them against the network weights in 2d. Based on this, explain your observations for the network behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdv2b8Mroe7F",
        "outputId": "40af019e-1d7e-4aab-a5f2-02a28beffa3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "[[0.5507979  0.70814782]]\n",
            "\n",
            "Network outputs after training:\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.06]\n",
            "Target: 0.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Target: 0.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Initial Weights:\n",
            "[[0.8204799  1.02328942]]\n"
          ]
        }
      ],
      "source": [
        "#Implementation for 3b. (same as 2d. but with change of one argument)\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "#Define a variable for input dimension\n",
        "input_dimension = 2\n",
        "\n",
        "#Define a variable for output dimension\n",
        "output_dimension = 1\n",
        "\n",
        "#Define a variable for voltage decay\n",
        "v_decay = 0.50\n",
        "\n",
        "#Define a variable for voltage threshold\n",
        "vth = 0.80\n",
        "\n",
        "#Define a variable for snn timesteps\n",
        "snn_timesteps = 50\n",
        "\n",
        "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
        "input_2_output_weight = np.random.rand(output_dimension,input_dimension)\n",
        "\n",
        "#print the initial weights\n",
        "print(\"Initial Weights:\")\n",
        "print(input_2_output_weight)\n",
        "\n",
        "#Initialize an snn using the arguments defined above\n",
        "snn = SNN(\n",
        "    input_2_output_weight,\n",
        "    input_dimension,\n",
        "    output_dimension,\n",
        "    v_decay,\n",
        "    vth,\n",
        "    snn_timesteps\n",
        ")\n",
        "\n",
        "#Get the training data for AND gate using the function defined in 2a.\n",
        "train_data = genANDTrainData(snn_timesteps)\n",
        "\n",
        "#Train the network using the function defined in 2c. with the appropriate arguments\n",
        "hebbian(\n",
        "    snn,\n",
        "    train_data,\n",
        "    lr=1e-3,\n",
        "    epochs=10*20\n",
        ")\n",
        "\n",
        "#Test the trained network and print the network output for all 4 cases.\n",
        "print(\"\\nNetwork outputs after training:\")\n",
        "for data in train_data:\n",
        "    input_spikes, target_output = data\n",
        "    output = snn(input_spikes)\n",
        "    print(f\"Target: {target_output.mean()}, Predicted Output: {np.round(output)}, Actual Output: {output}\")\n",
        "\n",
        "#Print Final Network Weights\n",
        "print(\"Initial Weights:\")\n",
        "print(snn.input_2_output_connection.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVneWi6_oe7F"
      },
      "source": [
        "## Answer 3b.\n",
        "No, with 20 times as many epochs, the network cannot still learn the AND gate correctly. We have a few observations:\n",
        "- Initial Weights: `[[0.5507979  0.70814782]]`\n",
        "- Final Weights (10 Epochs): `[[0.5639899  0.72116782]]`\n",
        "- Final Weights (10*20 Epochs): `[[0.8204799  1.02328942]]`\n",
        "\n",
        "We notice that the simplest Hebbian Learning rule updates the weight as $w_{new}= w_{old} + \\Delta w$, where $\\Delta w = xy$, the correlation between the input and the output. But notice, that this term $\\Delta w$ is always non-negative. Therefore, as we increase the number of epochs, the weights also correspondingly increase without bound, towards $\\infty$. Hence, after a certain number of epochs, the simplest Hebbian Learning rule is not meaningful in learning structure. The appropriate next question is, how can we bound the updates?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5NLJ6faoe7F"
      },
      "source": [
        "## 3c.\n",
        "Based on your observations and response in 3b., can you explain another limitation of hebbian learning rule w.r.t. weight growth? Can you also suggest a possible remedy for it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJo_gyjRoe7F"
      },
      "source": [
        "## Answer 3c.\n",
        "The simplest Hebbian Learning rule does not have a mechanism for forgetting or weight decay. Currently, the weights can only increase or remain the same based on the input/output firing rate correlations. This is an issue because the learning mechanism is not dynamic, and may update the weights due to spurious correlations between input/output firing rates. A solution might be to decay the current weights by some factor $\\alpha$. So instead of having the update rule be $\\Delta w = \\eta \\cdot xy$, we can have a decay mechanism with $\\Delta w = \\eta \\cdot xy - \\alpha \\cdot y^2 w$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXr7bV5-oe7F"
      },
      "source": [
        "## 3d.\n",
        "To resolve the issues with hebbian learning, one possibility is Oja's rule. In this exercise, you will implement and train an SNN using Oja's learning rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q7kUNAgUoe7F"
      },
      "outputs": [],
      "source": [
        "def oja(network, train_data, lr=1e-5, epochs=10):\n",
        "    \"\"\"\n",
        "    Function to train a network using Hebbian learning rule\n",
        "        Args:\n",
        "            network (SNN): SNN network object\n",
        "            train_data (list): training data\n",
        "            lr (float): learning rate\n",
        "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples.\n",
        "\n",
        "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights.\n",
        "    \"\"\"\n",
        "\n",
        "    #iterate over the epochs\n",
        "    for ee in range(epochs):\n",
        "        #iterate over all samples in train_data\n",
        "        for data in train_data:\n",
        "\n",
        "            input_spikes, _ = data\n",
        "\n",
        "            #compute the firing rate for the input\n",
        "            input_fr = np.mean(input_spikes, axis=1)\n",
        "\n",
        "            #compute the firing rate for the output\n",
        "            output_fr = np.mean(network(input_spikes))\n",
        "\n",
        "            #compute the weight increment\n",
        "            correlation = input_fr * output_fr\n",
        "            weight_increment = lr * (correlation - output_fr**2 * network.input_2_output_connection.weights)\n",
        "\n",
        "            #increment the weight\n",
        "            network.input_2_output_connection.weights += weight_increment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQUMzsBQoe7F"
      },
      "source": [
        "Now, test your implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkcxoN4Doe7G",
        "outputId": "96f79d81-b0e7-42c3-989b-7d9372255b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "[[0.5507979  0.70814782]]\n",
            "\n",
            "Network outputs after training:\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.06]\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.46]\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.48]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Initial Weights:\n",
            "[[0.65791595 0.77637841]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "#Define a variable for input dimension\n",
        "input_dimension = 2\n",
        "\n",
        "#Define a variable for output dimension\n",
        "output_dimension = 1\n",
        "\n",
        "#Define a variable for voltage decay\n",
        "v_decay = 0.50\n",
        "\n",
        "#Define a variable for voltage threshold\n",
        "vth = 0.80\n",
        "\n",
        "#Define a variable for snn timesteps\n",
        "snn_timesteps=50\n",
        "\n",
        "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
        "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
        "\n",
        "#print the initial weights\n",
        "print(\"Initial Weights:\")\n",
        "print(input_2_output_weight)\n",
        "\n",
        "#Initialize an snn using the arguments defined above\n",
        "snn = SNN(input_2_output_weight,\n",
        "          input_dimension,\n",
        "          output_dimension,\n",
        "          v_decay,\n",
        "          vth,\n",
        "          snn_timesteps)\n",
        "\n",
        "#Get the training data for AND gate using the function defined in 2a.\n",
        "train_data = genANDTrainData(snn_timesteps)\n",
        "\n",
        "#Train the network using the function defined in 3d. with the appropriate arguments\n",
        "oja(\n",
        "    snn,\n",
        "    train_data,\n",
        "    lr=1e-3,\n",
        "    epochs=10*20\n",
        ")\n",
        "\n",
        "#Test the trained network and print the network output for all 4 cases.\n",
        "print(\"\\nNetwork outputs after training:\")\n",
        "for data in train_data:\n",
        "    input_spikes, target_output = data\n",
        "    output = snn(input_spikes)\n",
        "    print(f\"Target: {target_output.mean()}, Predicted Output: {np.round(output)}, Actual Output: {output}\")\n",
        "\n",
        "#Print Final Network Weights\n",
        "print(\"Initial Weights:\")\n",
        "print(snn.input_2_output_connection.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lrjv_Tzoe7G"
      },
      "source": [
        "# Question 4: Spike-time dependent plasticity (STDP)\n",
        "\n",
        "## 4a.\n",
        "What is the limitation with hebbian learning that STDP aims to resolve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4fxjkwpoe7G"
      },
      "source": [
        "## Answer 4a.\n",
        "Hebbian learning updates synaptic weights based on correlations between pre-synaptic and post-synaptic activity. However, these updates don't take into account timing (i.e. causation) between presynaptic firing and postsynaptic firing. STDP strengthens connections when presynaptic firing precedes postsynaptic firing and weakens them if postsynaptic firing precedes presynaptic firing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1rA0Hgyoe7G"
      },
      "source": [
        "## 4b.\n",
        "Describe the algorithm to train a network using STDP learning rule. You do not need to describe encoding here. Your algorithm should be such that its naturally translatable to a program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4cHdMRoe7G"
      },
      "source": [
        "## Answer 4b.\n",
        "Parameters: $A^{+}$ and $A^{-}$ are the amplitudes of potentiation and depression. $\\tau_{+}$ and $\\tau_{-}$ are the time constants for potentiation and depression. $w_{min}$ and $w_{max}$ make sure that the updates are clipped and bound within the range $[w_{min}, w_{max}]$.\n",
        "\n",
        "1. Record all the spike times in neurons $(i, j)$ as $t_{i}$ and $t_{j}$\n",
        "2. Compute $\\Delta t = t_{j} - t_{i}$ for all spike times, which is $t$\n",
        "3. Update the weights as $w_{new} = \\begin{cases} \\max(w_{min}, w_{old} + \\eta \\cdot \\Delta w) & \\text{if } \\Delta w > 0 \\\\ \\min(w_{max}, w_{old} + \\eta \\cdot \\Delta w) & \\text{if } \\Delta w < 0 \\end{cases}$\n",
        "4. Then, update the weights as $w_{new} = w_{old} + \\eta \\cdot \\Delta w \\cdot b$\n",
        "5.Repeat this for all neuron pairs $i$ for $k$ epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_uwzqqIoe7G"
      },
      "source": [
        "## 4c.\n",
        "In this exercise, you will implement the STDP learning algorithm to train a network. STDP has many different flavors. For this exercise, we will use the learning rule defined in: https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.330110021. Pay special attention to Equations 2 and 3.\n",
        "\n",
        "Below is the class definition for STDP learning algorithm. Your task is to fill in the components so that the weights are updated in the right manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B6DLsSwuoe7G"
      },
      "outputs": [],
      "source": [
        "class STDP():\n",
        "    \"\"\"Train a network using STDP learning rule\"\"\"\n",
        "    def __init__(self, network, A_plus, A_minus, tau_plus, tau_minus, lr, snn_timesteps=20, epochs=30, w_min=0, w_max=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            network (SNN): network which needs to be trained\n",
        "            A_plus (float): STDP hyperparameter\n",
        "            A_minus (float): STDP hyperparameter\n",
        "            tau_plus (float): STDP hyperparameter\n",
        "            tau_minus (float): STDP hyperparameter\n",
        "            lr (float): learning rate\n",
        "            snn_timesteps (int): SNN simulation timesteps\n",
        "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples.\n",
        "            w_min (float): lower bound for the weights\n",
        "            w_max (float): upper bound for the weights\n",
        "        \"\"\"\n",
        "        self.network = network\n",
        "        self.A_plus = A_plus\n",
        "        self.A_minus = A_minus\n",
        "        self.tau_plus = tau_plus\n",
        "        self.tau_minus = tau_minus\n",
        "        self.snn_timesteps = snn_timesteps\n",
        "        self.lr = lr\n",
        "        self.time = np.arange(0, self.snn_timesteps, 1)\n",
        "        self.sliding_window = np.arange(-4, 4, 1) #defines a sliding window for STDP operation.\n",
        "        self.epochs = epochs\n",
        "        self.w_min = w_min\n",
        "        self.w_max = w_max\n",
        "\n",
        "    def update_weights(self, t, i):\n",
        "        \"\"\"\n",
        "        Function to update the network weights using STDP learning rule\n",
        "\n",
        "        Args:\n",
        "            t (int): time difference between postsynaptic spike and a presynaptic spike in a sliding window\n",
        "            i(int): index of the presynaptic neuron\n",
        "\n",
        "        Fill the details of STDP implementation\n",
        "        \"\"\"\n",
        "        #compute delta_w for positive time difference\n",
        "        if t>0:\n",
        "          delta_w = self.A_plus*np.exp(-t/self.tau_plus)\n",
        "\n",
        "        #compute delta_w for negative time difference\n",
        "        else:\n",
        "          delta_w = -self.A_minus*np.exp(-t/self.tau_minus)\n",
        "\n",
        "        #update the network weights if weight increment is negative\n",
        "        if delta_w < 0:\n",
        "          self.network.input_2_output_connection.weights[:, i] = np.maximum(self.w_min, self.network.input_2_output_connection.weights[:, i] + self.lr * delta_w)\n",
        "\n",
        "        #update the network weights if weight increment is positive\n",
        "        elif delta_w > 0:\n",
        "          self.network.input_2_output_connection.weights[:, i] = np.minimum(self.w_max, self.network.input_2_output_connection.weights[:, i] + self.lr * delta_w)\n",
        "\n",
        "    def train_step(self, train_data_sample):\n",
        "        \"\"\"\n",
        "        Function to train the network for one training sample using the update function defined above.\n",
        "\n",
        "        Args:\n",
        "            train_data_sample (list): a sample from the training data\n",
        "\n",
        "        This function is complete. You do not need to do anything here.\n",
        "        \"\"\"\n",
        "        input = train_data_sample[0]\n",
        "        output = train_data_sample[1]\n",
        "        for t in self.time:\n",
        "            if output[t] == 1:\n",
        "                for i in range(2):\n",
        "                    for t1 in self.sliding_window:\n",
        "                        if (0<= t + t1 < self.snn_timesteps) and (t1!=0) and (input[i][t+t1] == 1):\n",
        "                            self.update_weights(t1, i)\n",
        "\n",
        "    def train(self, training_data):\n",
        "        \"\"\"\n",
        "        Function to train the network\n",
        "\n",
        "        Args:\n",
        "            training_data (list): training data\n",
        "\n",
        "        This function is complete. You do not need to do anything here.\n",
        "        \"\"\"\n",
        "        for ee in range(self.epochs):\n",
        "            for train_data_sample in training_data:\n",
        "                self.train_step(train_data_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4HFcHdaoe7H"
      },
      "source": [
        "Let's test the implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiklbYQwoe7H",
        "outputId": "aa733c18-66bd-463e-ea3c-003148e303da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "[[0.5507979  0.70814782]]\n",
            "\n",
            "Network outputs after training:\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.06]\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.46]\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.48]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "\n",
            "Final weights:\n",
            "[[0.53563823 0.69298815]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "#Define a variable for input dimension\n",
        "input_dimension=2\n",
        "\n",
        "#Define a variable for output dimension\n",
        "output_dimension=1\n",
        "\n",
        "#Define a variable for voltage decay\n",
        "v_decay=0.5\n",
        "\n",
        "#Define a variable for voltage threshold\n",
        "vth=0.8\n",
        "\n",
        "#Define a variable for snn timesteps\n",
        "snn_timesteps=50\n",
        "\n",
        "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
        "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
        "print(\"Initial Weights:\")\n",
        "print(input_2_output_weight)\n",
        "\n",
        "#Initialize an snn using the arguments defined above\n",
        "snn = SNN(input_2_output_weight,\n",
        "          input_dimension,\n",
        "          output_dimension,\n",
        "          v_decay,\n",
        "          vth,\n",
        "          snn_timesteps)\n",
        "\n",
        "#Get the training data for AND gate using the function defined in 2a.\n",
        "train_data = genANDTrainData(snn_timesteps)\n",
        "\n",
        "#Create an object of STDP class with appropriate arguments\n",
        "stdp = STDP(\n",
        "    network=snn,\n",
        "    A_plus = 0.10,\n",
        "    A_minus = 0.10,\n",
        "    tau_plus = 20,\n",
        "    tau_minus=20,\n",
        "    lr=1e-5,\n",
        "    snn_timesteps=snn_timesteps,\n",
        "    epochs=10*20,\n",
        "    w_min=0,\n",
        "    w_max=1,\n",
        ")\n",
        "\n",
        "#Train the network using STDP\n",
        "stdp.train(train_data)\n",
        "\n",
        "#Test the trained network and print the network output for all 4 cases.\n",
        "print(\"\\nNetwork outputs after training:\")\n",
        "for data in train_data:\n",
        "    input_spikes, target_output = data\n",
        "    output = snn(input_spikes)\n",
        "    print(f\"Target: {target_output.mean()}, Predicted Output: {np.round(output)}, Actual Output: {output}\")\n",
        "print(\"\\nFinal weights:\")\n",
        "print(snn.input_2_output_connection.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ame9Eq7ioe7H"
      },
      "source": [
        "# Question 5: OR Gate\n",
        "Can you train the network with the same architecture in Q2-4 for learning the OR gate. You will need to create another function called genORTrainData. Then create an SNN and train it using STDP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rSA5vSqpoe7H"
      },
      "outputs": [],
      "source": [
        "#Write your implementation of genORTrainData here.\n",
        "\n",
        "def genORTrainData(snn_timestep):\n",
        "    \"\"\"\n",
        "    Function to generate the training data for AND\n",
        "        Args:\n",
        "            snn_timestep (int): timesteps for SNN simulation\n",
        "        Return:\n",
        "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
        "\n",
        "        Write the expressions for encoding 0 and 1. Then append all 4 cases of AND gate to the list train_data\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize an empty list for train data\n",
        "    train_data = []\n",
        "\n",
        "    #encode 0. Numpy random choice function might be useful here.\n",
        "    encode_0 = np.random.choice([0, 1], size=snn_timestep, p=[0.90, 0.10])\n",
        "\n",
        "    #encode 1. Numpy random choice function might be useful here.\n",
        "    encode_1 = np.random.choice([0, 1], size=snn_timestep, p=[0.10, 0.90])\n",
        "\n",
        "    #Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here.\n",
        "    train_data.append((np.stack((encode_0, encode_0)), np.zeros(snn_timestep)))\n",
        "    train_data.append((np.stack((encode_0, encode_1)), np.ones(snn_timestep)))\n",
        "    train_data.append((np.stack((encode_1, encode_0)), np.ones(snn_timestep)))\n",
        "    train_data.append((np.stack((encode_1, encode_1)), np.ones(snn_timestep)))\n",
        "\n",
        "    return train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuNH5uu2oe7H",
        "outputId": "7086f6c2-5369-49a3-fc03-4f1294dadcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "[[0.5507979  0.70814782]]\n",
            "\n",
            "Network outputs after training:\n",
            "Target: 0.0, Predicted Output: [0.], Actual Output: [0.06]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "Target: 1.0, Predicted Output: [1.], Actual Output: [0.92]\n",
            "\n",
            "Final weights:\n",
            "[[0.54926946 0.70661938]]\n"
          ]
        }
      ],
      "source": [
        "#Train the network for OR gate here using the implementation from 4c.\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "#Define a variable for input dimension\n",
        "input_dimension=2\n",
        "\n",
        "#Define a variable for output dimension\n",
        "output_dimension=1\n",
        "\n",
        "#Define a variable for voltage decay\n",
        "v_decay=0.5\n",
        "\n",
        "#Define a variable for voltage threshold\n",
        "vth=0.5\n",
        "\n",
        "#Define a variable for snn timesteps\n",
        "snn_timesteps=50\n",
        "\n",
        "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
        "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
        "print(\"Initial Weights:\")\n",
        "print(input_2_output_weight)\n",
        "\n",
        "#Initialize an snn using the arguments defined above\n",
        "snn = SNN(input_2_output_weight,\n",
        "          input_dimension,\n",
        "          output_dimension,\n",
        "          v_decay,\n",
        "          vth,\n",
        "          snn_timesteps)\n",
        "\n",
        "#Get the training data for AND gate using the function defined in 2a.\n",
        "train_data = genORTrainData(snn_timesteps)\n",
        "\n",
        "#Create an object of STDP class with appropriate arguments\n",
        "stdp = STDP(\n",
        "    network=snn,\n",
        "    A_plus = 0.12,\n",
        "    A_minus = 0.05,\n",
        "    tau_plus = 12,\n",
        "    tau_minus=5,\n",
        "    lr=1e-5,\n",
        "    snn_timesteps=snn_timesteps,\n",
        "    epochs=75,\n",
        "    w_min=0,\n",
        "    w_max=1,\n",
        ")\n",
        "\n",
        "#Train the network using STDP\n",
        "stdp.train(train_data)\n",
        "\n",
        "#Test the trained network and print the network output for all 4 cases.\n",
        "print(\"\\nNetwork outputs after training:\")\n",
        "for data in train_data:\n",
        "    input_spikes, target_output = data\n",
        "    output = snn(input_spikes)\n",
        "    print(f\"Target: {target_output.mean()}, Predicted Output: {np.round(output)}, Actual Output: {output}\")\n",
        "print(\"\\nFinal weights:\")\n",
        "print(snn.input_2_output_connection.weights)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}